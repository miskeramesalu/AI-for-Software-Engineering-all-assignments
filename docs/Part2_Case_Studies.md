# Part 2: Case Study Analysis

## Case 1: Amazon Hiring Tool

### 1. Source of Bias
- The training data was dominated by **male applicants**.
- Model learned to penalize words associated with women.
- Historical hiring patterns were replicated as algorithmic bias.

### 2. Fixes
1. Rebalance training dataset (gender-neutral).  
2. Remove gender-correlated features.  
3. Implement fairness constraints (equal opportunity, disparate impact).

### 3. Fairness Metrics
- Disparate Impact Ratio  
- Equal Opportunity Difference  
- TPR/FPR parity  

---

## Case 2: Facial Recognition in Policing

### Ethical Risks
- Wrongful arrests  
- Racial profiling  
- Privacy invasion  
- Loss of trust in law enforcement  

### Policies
- Mandatory accuracy benchmarks  
- Human-in-the-loop verification  
- Transparent auditing and reporting  
- Restricted usage (not for live public surveillance)